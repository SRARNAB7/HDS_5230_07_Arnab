{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3Q+O4ZTmmtcP8pHqn6ZmR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SRARNAB7/HDS_5230_07_Arnab/blob/main/Week%2009/Week_09_Assignment_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Among the different classification models included in the Python notebook, which model had the best overall performance? Support your response by referencing appropriate evidence."
      ],
      "metadata": {
        "id": "Bq32W2Ei3dtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model     Train     Test\n",
        "\n",
        "Logistic  0.7333    0.718\n",
        "\n",
        "Null 0.6467 0.608\n",
        "\n",
        "Logistic_L1_C_1 0.732 0.716\n",
        "\n",
        "Logistic_L1_C_01 0.726 0.706\n",
        "\n",
        "Logistic_L1_C_10 0.7347 0.718\n",
        "\n",
        "Logistic_L1_C_auto 0.7233 0.708\n",
        "\n",
        "Logistic_SL1_C_auto 0.7307 0.714\n",
        "\n",
        "RandomForest_noCV 0.9993 0.69\n",
        "\n",
        "RandomForest_CV 0.9987 0.702\n",
        "\n",
        "RandomForest_CV2 0.7273 0.702"
      ],
      "metadata": {
        "id": "A1cJ4gEo3iMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the results presented in the screenshot, the Logistic regression model with L1 penalty and C=10 (Logistic_L1_C_10) demonstrated the best overall performance. It achieved the highest test accuracy of 0.718, which is a key indicator of how well the model generalizes to new, unseen data. In contrast, the Random Forest (no CV) model had an almost perfect training accuracy of 0.9993, but its test accuracy dropped sharply to 0.686, suggesting significant overfitting.\n",
        "\n",
        "The Logistic_L1_C_10 model showed a strong balance between training and testing performance, with a training accuracy of 0.7347 and a test accuracy of 0.718. This close alignment indicates that the model generalizes well without being overly fitted to the training data. Therefore, among all the classification models evaluated, Logistic_L1_C_10 proved to be the most reliable and generalizable."
      ],
      "metadata": {
        "id": "ULY_dyK13sRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv4va_Yc3Pek"
      },
      "outputs": [],
      "source": [
        "## Import Modules\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from patsy import dmatrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import sklearn\n",
        "from sklearn import datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Set default figure size to be larger\n",
        "## this may only work in matplotlib 2.0+!\n",
        "matplotlib.rcParams['figure.figsize'] = [10.0,6.0]\n",
        "## Enable multiple outputs from jupyter cells\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "rC_TtTQ63xzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get Version information\n",
        "print(sys.version)\n",
        "print(\"Pandas version: {0}\".format(pd.__version__))\n",
        "print(\"Matplotlib version: {0}\".format(matplotlib.__version__))\n",
        "print(\"Numpy version: {0}\".format(np.__version__))\n",
        "print(\"SciKitLearn version: {0}\".format(sklearn.__version__))"
      ],
      "metadata": {
        "id": "CrmIlWQX3zaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the working directory Set the working directory to make paths easier :)"
      ],
      "metadata": {
        "id": "hWdRvZcK32ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working Directory\n",
        "import os\n",
        "print(\"My working directory:\\n\" + os.getcwd())\n",
        "# Set Working Directory\n",
        "os.chdir(\".\")\n",
        "print(\"My new working directory:\\n\" + os.getcwd())"
      ],
      "metadata": {
        "id": "a5-AZyS235kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Patient Mortality Dataset We will use a dataset with a binary outcome of mortality as a motivating example.\n",
        "\n",
        "This is a dataset of patients demographics and disease status, with mortality indicated. The dataset is here:\n",
        "\n",
        "data\\healthcare\\patientAnalyticFile.csv\n",
        "\n",
        "In practice, you most likely would have created a dataset like this from multiple other files after cleaning, reshaping, and joining them.\n",
        "\n",
        "You can generalize this setup to any situation with a binary outcome, such as estimating the probability of a customer filing a warranty claim, or the probability of a transaction being fraudulent.\n",
        "\n",
        "We will first import this dataset and examine the potential variables to use in our classification algorithm."
      ],
      "metadata": {
        "id": "6kgLWOQy38gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set print limits\n",
        "pd.options.display.max_rows = 10\n",
        "## Import Data\n",
        "df_patient = \\\n",
        " pd.read_csv('./PatientAnalyticFile.csv')\n",
        "df_patient"
      ],
      "metadata": {
        "id": "57qwzfBd3-IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to make a variable to indicate mortality. We can do that based on the abscence of 'date of death':"
      ],
      "metadata": {
        "id": "m-sztRow4Bjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mortality variable\n",
        "df_patient['mortality'] = \\\n",
        "    np.where(df_patient['DateOfDeath'].isnull(),\n",
        "             0,1)\n",
        "# Examine\n",
        "df_patient['mortality']"
      ],
      "metadata": {
        "id": "zjHj7Gfb3_v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient['mortality'].describe()"
      ],
      "metadata": {
        "id": "G-HkHP6x4E0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient.describe()"
      ],
      "metadata": {
        "id": "G7DbDETV4FWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient.dtypes"
      ],
      "metadata": {
        "id": "YDgQ0NRs4G7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We should change date of birth to be an actual date and calculate age if we want to include it in the model:"
      ],
      "metadata": {
        "id": "l-FFU6yl4Jzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert dateofBirth to date\n",
        "df_patient['DateOfBirth'] = \\\n",
        "    pd.to_datetime(df_patient['DateOfBirth'])\n",
        "# Calculate age in years as of 2015-01-01\n",
        "df_patient['Age_years'] = \\\n",
        "    ((pd.to_datetime('2015-01-01') - df_patient['DateOfBirth']).dt.days/365.25)\n",
        "df_patient['Age_years'].describe()"
      ],
      "metadata": {
        "id": "9SmYdVop4IW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Patsy to Create the Model Matrices We typically start out with a pandas dataframe for manipulation purposes, then we will use this dataframe as the input to the machine learning library. I created a pandas dataframe above to replicate this process. We will use the dmatrices function from the patsy library to easily generate the design matrices for the machine learning algorithms representing the inputs. THis handles the following:\n",
        "\n",
        "drops rows with missing data construct one-hot encoding for categorical variables optionally adds constant intecercept"
      ],
      "metadata": {
        "id": "kYIJH8ko4Ojt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_patient.columns"
      ],
      "metadata": {
        "id": "kWmNj84w4MS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create formula for all variables in model\n",
        "vars_remove = ['PatientID','First_Appointment_Date','DateOfBirth',\n",
        "               'Last_Appointment_Date','DateOfDeath','mortality']\n",
        "vars_left = set(df_patient.columns) - set(vars_remove)\n",
        "formula = \"mortality ~ \" + \" + \".join(vars_left)\n",
        "formula"
      ],
      "metadata": {
        "id": "AH0QhP8V4Qfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Next, fitting a series of logistic regression models, without regularization. Each model should use the same set of predictors (all of the relevant predictors in the dataset) and should use the entire dataset, rather than a fraction of it. Use a randomly chosen 80% proportion of observations for training and the remaining for checking the generalizable performance (i.e., performance on the holdout subset). Be sure to ensure that the training and holdout subsets are identical across all models. Each model should choose a different solver.**"
      ],
      "metadata": {
        "id": "i-UQAMBX4T5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y, X = dmatrices(formula, df_patient)"
      ],
      "metadata": {
        "id": "c5hnoFZa4gx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "V-NVw_V68gej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "OLedOznB8g-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, np.ravel(Y), test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_iNx_JVH8iX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirming the Output Dimensions. The dimensions of the data are the same within test and train. The proportion should also be close to the test_size argument."
      ],
      "metadata": {
        "id": "3yZATN1R8sMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Confirm dimensions\n",
        "X_train.shape"
      ],
      "metadata": {
        "id": "ZJg9OECX8rhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "-Hn8Ve0Y8z25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "UwRgo_0f81AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "id": "UFkRS9HX82Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic Regression Models with Different Solvers that are not generalized. The different solvers are 'lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'**"
      ],
      "metadata": {
        "id": "UHZ_R1lp84yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# List of solvers that support penalty=None\n",
        "solvers = ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Loop through solvers\n",
        "for solver in solvers:\n",
        "    print(f\"\\nTraining with solver = {solver}\")\n",
        "\n",
        "    # Initialize model\n",
        "    clf = LogisticRegression(\n",
        "        penalty=None,\n",
        "        solver=solver,\n",
        "        fit_intercept=True,\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Time the fitting process\n",
        "    start_time = time.time()\n",
        "    clf.fit(X_train, y_train)\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Evaluate\n",
        "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"Solver\": solver,\n",
        "        \"Train Accuracy\": round(train_acc, 4),\n",
        "        \"Test Accuracy\": round(test_acc, 4),\n",
        "        \"Time (seconds)\": round(duration, 4)\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display table\n",
        "results_df.sort_values(by=\"Test Accuracy\", ascending=False, inplace=True)\n",
        "results_df.reset_index(drop=True, inplace=True)\n",
        "results_df"
      ],
      "metadata": {
        "id": "wRCGkHx69EYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Based on the results, which solver yielded the best results? Explain the basis for ranking the models - did you use training subset accuracy? Holdout subset accuracy? Time of execution? All three? Some combination of the three?**"
      ],
      "metadata": {
        "id": "23QUI4NZ9-cY"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "File #1 - OP_DTL_GNRL_PGYR2023_P01302025_{dateOfFilesExtraction-MMDDYYYY}.csv: This file contains the data set of General Payments reported for the 2023 program year. General Payments are defined as payments or other transfers of value made to a covered recipient (physician, non-physician practitioner or teaching hospital) that are not made in connection with a research agreement or research protocol.\n",
        "\n",
        "File #2 - OP_DTL_RSRCH_PGYR2023_P01302025_{dateOfFilesExtraction-MMDDYYYY}.csv: This file contains the data set of Research Payments reported for the 2023 program year. Research Payments are defined as payments or other transfers of value made in connection with a research agreement or research protocol.\n",
        "\n",
        "Covered Recipient Profile Supplement File\n",
        "\n",
        "The Covered Recipient Profile Supplement file contains information about physicians and non-physician practitioners who have been indicated as recipients of payments, other transfers of value, or ownership and investment interest in payment records, as well as physicians and non-physician practitioners who have been identified as principal investigators associated with research payment records published by Open Payments.\n",
        "\n",
        "This file contains only those physicians that have at least one published payment record in this cycle of the publication as of May 30, 2024. The criteria used by the Centers for Medicare and Medicaid Services (CMS) to determine which payment records are eligible for publication is available in the Open Payments Methodology and Data Dictionary Document. This document can be found on the Resources page of the Open Payments website (https://www.cms.gov/OpenPayments/Resources). The Methodology and Data Dictionary Document also includes information on the data collection and reporting methodology, data fields included in the files, and any notes or special considerations that users should be aware of."
      ],
      "metadata": {
        "id": "39P96uXOJTGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have taken in consideration the General Paymenst file, Research file. Since the nature of payments present in the General Payments file are eligible for payment. Moreover, the costs of the research is also eligible for payments. Hence they are considered."
      ],
      "metadata": {
        "id": "3l93_JVqJYTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code runs in a Databricks notebook and uses shell commands to automate the download and extraction of a ZIP file containing Open Payments data from the CMS (Centers for Medicare & Medicaid Services) website. Downloads a ZIP file from the CMS Open Payments website and saves it as openpayments_2023.zip in the /tmp folder on the Databricks driver node. The -L option ensures that if the URL redirects, the download still works. Unzips (extracts) the contents of that ZIP file into a new folder called /tmp/openpayments_2023."
      ],
      "metadata": {
        "id": "c0ZCO4BXJZmJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sM3we1NEJAUv"
      },
      "outputs": [],
      "source": [
        "%%sh\n",
        "# Download the CMS Open Payments ZIP file\n",
        "curl -L https://download.cms.gov/openpayments/PGYR2023_P01302025_01212025.zip -o /tmp/openpayments_2023.zip\n",
        "\n",
        "# Unzip the file to a directory\n",
        "unzip /tmp/openpayments_2023.zip -d /tmp/openpayments_2023"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line import os loads a built-in Python module called os, which helps work with files and directories.\n",
        "\n",
        "The line os.listdir(\"/tmp/openpayments_2023\") checks the /tmp/openpayments_2023 folder and returns a list of all the files inside it."
      ],
      "metadata": {
        "id": "tr8fw99bLI3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List the extracted files\n",
        "os.listdir(\"/tmp/openpayments_2023\")"
      ],
      "metadata": {
        "id": "pOAg1dO5LKRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the CSV file into a Spark DataFrame\n",
        "df = spark.read.csv(\"file:/tmp/openpayments_2023/OP_DTL_GNRL_PGYR2023_P01302025_01212025.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Saving it as a Delta table\n",
        "df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"General_2023_OpenPayments\")"
      ],
      "metadata": {
        "id": "whDyH904LPen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below query for checking whether all the rows have been loaded in the table from the file."
      ],
      "metadata": {
        "id": "ZI2ihGfRLR48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select count(*)\n",
        " from General_2023_OpenPayments"
      ],
      "metadata": {
        "id": "CCPiiLRnLSuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below query for checking the different types of Types Covered Recipient Type in th table. Since all the types may not be eligible for reiumbursement."
      ],
      "metadata": {
        "id": "M2sHQ67KLXib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select distinct Covered_Recipient_Type from General_2023_OpenPayments"
      ],
      "metadata": {
        "id": "_Z-dtJhuLf0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is for loading the Research file in a dataframe and then into a table."
      ],
      "metadata": {
        "id": "fG6gNIOLLjco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File location and type\n",
        "file_location = \"/FileStore/tables/OP_DTL_RSRCH_PGYR2023_P01302025_01212025-1.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"false\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "Wcir3Co_LkOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"Research_table\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ],
      "metadata": {
        "id": "DlnnJFnWLqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below query for checking whether all the rows have been loaded in the table from the file."
      ],
      "metadata": {
        "id": "2AfC1WM7LtJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select count(*) from Research_table;"
      ],
      "metadata": {
        "id": "ewfHodCNLtni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is for loading the Recepient file in a dataframe and then into a table."
      ],
      "metadata": {
        "id": "D_5qaDwjLxMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File location and type\n",
        "file_location = \"/FileStore/tables/OP_CVRD_RCPNT_PRFL_SPLMTL_P01302025_01212025.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"false\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "\n",
        "# The applied options are for CSV files. For other file types, these will be ignored.\n",
        "df = spark.read.format(file_type) \\\n",
        "  .option(\"inferSchema\", infer_schema) \\\n",
        "  .option(\"header\", first_row_is_header) \\\n",
        "  .option(\"sep\", delimiter) \\\n",
        "  .load(file_location)\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "id": "0LQurYp-LxtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a view or table\n",
        "\n",
        "temp_table_name = \"Recepient_table\"\n",
        "\n",
        "df.createOrReplaceTempView(temp_table_name)"
      ],
      "metadata": {
        "id": "al0NYH8_MA7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below query for checking whether all the rows have been loaded in the table from the file."
      ],
      "metadata": {
        "id": "RjsjhJGeMI0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select count(*) from Recepient_table"
      ],
      "metadata": {
        "id": "BLk3qlCYMKfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The General_2023_OpenPayments is joined with Recepient_table based on the Covered_Recipient_Profile_ID to have the records that are eligible for the reuimbursement."
      ],
      "metadata": {
        "id": "d0o2OAXyMOTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Loading the tables tables\n",
        "General_Open_Payments_df = spark.table(\"General_2023_OpenPayments\")\n",
        "recepient_df = spark.table(\"Recepient_table\")\n",
        "\n",
        "# To match the key column types match\n",
        "General_Open_Payments_df = General_Open_Payments_df.withColumn(\"Covered_Recipient_Profile_ID\", col(\"Covered_Recipient_Profile_ID\").cast(\"string\"))\n",
        "recepient_df = recepient_df.withColumn(\"Covered_Recipient_Profile_ID\", col(\"Covered_Recipient_Profile_ID\").cast(\"string\"))\n",
        "\n",
        "# Filtering Research using inner join, but only keeping Research columns\n",
        "General_Open_Payments_df_filtered = General_Open_Payments_df.join(\n",
        "    recepient_df.select(\"Covered_Recipient_Profile_ID\").dropna().distinct(),\n",
        "    on=\"Covered_Recipient_Profile_ID\",\n",
        "    how=\"inner\"\n",
        ").select(General_Open_Payments_df.columns)  # <- Only keeps Research_csv columns\n",
        "\n",
        "# Saving as new table\n",
        "General_Open_Payments_df_filtered.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"General_Open_Payments_df_filtered\")"
      ],
      "metadata": {
        "id": "WCQEiQ5QMPN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select distinct Covered_Recipient_Type\n",
        " from General_Open_Payments_df_filtered"
      ],
      "metadata": {
        "id": "f7ttDV9FMYfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is used to filter the unwanted Types from the Covered_Recipient_Type in order to clean the datasets"
      ],
      "metadata": {
        "id": "y9SfjGHSQQ_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Loading the table into a DataFrame\n",
        "df = spark.table(\"General_Open_Payments_df_filtered\")\n",
        "\n",
        "# Defining allowed values\n",
        "valid_types = [\n",
        "    \"Covered Recipient Physician\",\n",
        "    \"Covered Recipient Non-Physician Practitioner\",\n",
        "    \"Covered Recipient Physician/Covered Recipient Non-Physician Practitioner\"\n",
        "]\n",
        "\n",
        "# Filtering the DataFrame\n",
        "filtered_df = df.filter(col(\"Covered_Recipient_Type\").isin(valid_types))\n",
        "\n",
        "# Overwriting the table with filtered data\n",
        "filtered_df.write.mode(\"overwrite\").saveAsTable(\"General_Open_Payments_df_filtered\")"
      ],
      "metadata": {
        "id": "LjuL2641QRwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select distinct Covered_Recipient_Type\n",
        " from General_Open_Payments_df_filtered"
      ],
      "metadata": {
        "id": "PmJxRtVXQYMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Research table is joined with Recepient_table based on the Covered_Recipient_Profile_ID to have the records that are eligible for the reuimbursement."
      ],
      "metadata": {
        "id": "AGJnx5otQaGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Load tables\n",
        "Research_table_df = spark.table(\"Research_table\")\n",
        "recepient_df = spark.table(\"Recepient_table\")\n",
        "\n",
        "# Ensure key column types match\n",
        "Research_table_df = Research_table_df.withColumn(\"Covered_Recipient_Profile_ID\", col(\"Covered_Recipient_Profile_ID\").cast(\"string\"))\n",
        "recepient_df = recepient_df.withColumn(\"Covered_Recipient_Profile_ID\", col(\"Covered_Recipient_Profile_ID\").cast(\"string\"))\n",
        "\n",
        "# Filter Research using inner join, but only keep Research columns\n",
        "Research_table_df_filtered = Research_table_df.join(\n",
        "    recepient_df.select(\"Covered_Recipient_Profile_ID\").dropna().distinct(),\n",
        "    on=\"Covered_Recipient_Profile_ID\",\n",
        "    how=\"inner\"\n",
        ").select(Research_table_df.columns)  # <- Only keeps Research_csv columns\n",
        "\n",
        "# Save as new table\n",
        "Research_table_df_filtered.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"Research_table_df_filtered\")"
      ],
      "metadata": {
        "id": "tMUMtq8bQc_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select count(*) from Research_table_df_filtered;"
      ],
      "metadata": {
        "id": "hYON0wTIQiQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select distinct Covered_Recipient_Type\n",
        " from Research_table_df_filtered"
      ],
      "metadata": {
        "id": "VJbrgut0QrXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below three code blocks is written to create a table with few selected columns that are required for the assignment. Thus it will save execution time. A union is performed on the tables to combine the records based on the y-axis. Thus all the records will be there in one table from where we can answer the questions of the assignment."
      ],
      "metadata": {
        "id": "KszJEroOQtdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Select and rename the necessary columns since union will be perfromed so the colum names should be same\n",
        "selected_df = Research_table_df_filtered.select(\n",
        "    \"Covered_Recipient_Profile_ID\",\n",
        "    \"Covered_Recipient_NPI\",\n",
        "    \"Covered_Recipient_First_Name\",\n",
        "    \"Covered_Recipient_Middle_Name\",\n",
        "    \"Covered_Recipient_Last_Name\",\n",
        "    \"Covered_Recipient_Specialty_1\",\n",
        "    \"Total_Amount_of_Payment_USDollars\",\n",
        "    col(\"Form_of_Payment_or_Transfer_of_Value\").alias(\"Nature_of_Payment_or_Transfer_of_Value\")\n",
        ")\n",
        "\n",
        "# Save as a new Delta table with renamed column\n",
        "selected_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"Research_Payments_Modified\")"
      ],
      "metadata": {
        "id": "qi6FNOm2QuQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the columns needed\n",
        "selected_df = General_Open_Payments_df_filtered.select(\n",
        "    \"Covered_Recipient_Profile_ID\",\n",
        "    \"Covered_Recipient_NPI\",\n",
        "    \"Covered_Recipient_First_Name\",\n",
        "    \"Covered_Recipient_Middle_Name\",\n",
        "    \"Covered_Recipient_Last_Name\",\n",
        "    \"Covered_Recipient_Specialty_1\",\n",
        "    \"Total_Amount_of_Payment_USDollars\",\n",
        "    \"Nature_of_Payment_or_Transfer_of_Value\"\n",
        ")\n",
        "\n",
        "# Save the selected data as a new Delta table\n",
        "selected_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"General_Open_Payments_Modified\")"
      ],
      "metadata": {
        "id": "xQO_QmZQRHI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code is for performing union of the two tables"
      ],
      "metadata": {
        "id": "SSU9NukxRJlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading both tables\n",
        "research_df = spark.table(\"Research_Payments_Modified\")\n",
        "general_df = spark.table(\"General_Open_Payments_Modified\")\n",
        "\n",
        "# Performing the union\n",
        "union_df = research_df.unionByName(general_df)\n",
        "\n",
        "# Showing the result\n",
        "union_df.show()\n",
        "\n",
        "# Saving the union result as a new Delta table\n",
        "union_df.write \\\n",
        "    .format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .saveAsTable(\"Combined_Research_General_Payments\")"
      ],
      "metadata": {
        "id": "Dzn86_zQRKWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%sql\n",
        "Select * from Combined_Research_General_Payments"
      ],
      "metadata": {
        "id": "_cy2BSLnRPLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. What is the Nature of Payments with reimbursement amounts greater than $1,000 ordered by count?\n",
        "\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Loading the unified payments table\n",
        "df = spark.table(\"Combined_Research_General_Payments\")\n",
        "\n",
        "# Filtering payments greater than $1,000 and group by Nature of Payment\n",
        "nature_over_1000 = df.filter(col(\"Total_Amount_of_Payment_USDollars\") > 1000) \\\n",
        "    .groupBy(\"Nature_of_Payment_or_Transfer_of_Value\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc())\n",
        "\n",
        "# Showing the result\n",
        "nature_over_1000.show()"
      ],
      "metadata": {
        "id": "gs2Fyt4_RQC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top category is \"Compensation for services other than consulting\" with 161,476 occurrences, followed by \"Consulting Fee\" with 104,564 occurrences. Other notable categories include \"Travel and Lodging\" with 24,711, \"Honoraria\" with 13,741, \"Education\" with 12,503, and \"Royalty or License\" with 10,573."
      ],
      "metadata": {
        "id": "lKznIi7fRSZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. What are the top ten Nature of Payments by count?\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Load the unified payments table\n",
        "df = spark.table(\"Combined_Research_General_Payments\")\n",
        "\n",
        "# Group by Nature of Payment and count, then get top 10\n",
        "top_nature_by_count = df.groupBy(\"Nature_of_Payment_or_Transfer_of_Value\") \\\n",
        "    .count() \\\n",
        "    .orderBy(col(\"count\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Show the result\n",
        "top_nature_by_count.show()"
      ],
      "metadata": {
        "id": "VlRD53zRRVCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top ten nature of payments by count are led by Food and Beverage, which is by far the most frequent, with 13,378,081 instances. This is followed by Travel and Lodging, recorded 545,048 times, and Compensation for services other than consulting, with 230,121 instances. Consulting Fee appears next with 169,540 counts, and Education follows closely with 159,397 instances. Other notable categories include Gift with 31,695 counts, Honoraria with 20,214, and Royalty or License with 14,007.In-kind items and services, reported 13,639 times, and Cash or cash equivalent, which appears 12,646 times. These figures suggest that the majority of payments are related to everyday interactions like meals, travel, and professional services."
      ],
      "metadata": {
        "id": "CVzHISAeRWdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What are the top ten Nature of Payments by total amount?\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "# Load the unified payments table\n",
        "df = spark.table(\"Combined_Research_General_Payments\")\n",
        "\n",
        "# Group by Nature of Payment and sum total amount\n",
        "top_nature_by_amount = df.groupBy(\"Nature_of_Payment_or_Transfer_of_Value\") \\\n",
        "    .agg(spark_sum(\"Total_Amount_of_Payment_USDollars\").alias(\"Total_Amount\")) \\\n",
        "    .orderBy(col(\"Total_Amount\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Show the result\n",
        "top_nature_by_amount.show()"
      ],
      "metadata": {
        "id": "k6pXc2KyRYW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The category with the highest total payment amount is Royalty or License, followed by Compensation for services other than consulting. In third place is Consulting Fee. Next is Food and Beverage, followed by Travel and Lodging. Acquisitions comes after that, while Cash or cash equivalent ranks next in line. Education follows, with Honoraria close behind. Finally, another category of Compensation for services completes the top ten. These results indicate that while some categories like Food and Beverage and Travel and Lodging occur more frequently, categories such as Royalty or License and Consulting Fee are associated with higher overall payment amounts."
      ],
      "metadata": {
        "id": "TZWOVDhbRauY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#4. What are the top ten physician specialties by total amount?\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum\n",
        "\n",
        "# Load the unified payments table\n",
        "df = spark.table(\"Combined_Research_General_Payments\")\n",
        "\n",
        "# Group by specialty and sum the total payment\n",
        "top_specialties = df.groupBy(\"Covered_Recipient_Specialty_1\") \\\n",
        "    .agg(spark_sum(\"Total_Amount_of_Payment_USDollars\").alias(\"Total_Amount\")) \\\n",
        "    .orderBy(col(\"Total_Amount\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Show the result\n",
        "top_specialties.show(truncate=False)"
      ],
      "metadata": {
        "id": "v8l4Co02RbVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the results, the physician specialty that received the highest total payment amount is Orthopaedic Surgery, which falls under Allopathic & Osteopathic Physicians. The second highest is Internal Medicine, followed by Neurology, categorized under Psychiatry & Neurology.\n",
        "\n",
        "Next is Neurological Surgery, followed by Dermatology. Specialists in Hematology & Oncology, a branch of Internal Medicine, come next, followed closely by Cardiovascular Disease, also under Internal Medicine.\n",
        "\n",
        "Adult Reconstructive Orthopaedic Surgery, a subspecialty of Orthopaedic Surgery, ranks next, followed by Psychiatry, which falls under Psychiatry & Neurology. Finally, general Surgery specialists complete the top ten.\n",
        "\n",
        "These results show that specialties in surgery, internal medicine, and neurology received the highest overall payment amounts."
      ],
      "metadata": {
        "id": "0BpfDPWeRc8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Who are the top ten physicians by total amount?\n",
        "from pyspark.sql.functions import col, sum as spark_sum, concat_ws\n",
        "\n",
        "# Load the unified payments table\n",
        "df = spark.table(\"Combined_Research_General_Payments\")\n",
        "\n",
        "# Create a full name column for readability\n",
        "df = df.withColumn(\n",
        "    \"Physician_Full_Name\",\n",
        "    concat_ws(\" \", col(\"Covered_Recipient_First_Name\"),col(\"Covered_Recipient_Middle_Name\"), col(\"Covered_Recipient_Last_Name\"))\n",
        ")\n",
        "\n",
        "# Group by physician name and sum total payments\n",
        "top_physicians = df.groupBy(\"Physician_Full_Name\") \\\n",
        "    .agg(spark_sum(\"Total_Amount_of_Payment_USDollars\").alias(\"Total_Amount\")) \\\n",
        "    .orderBy(col(\"Total_Amount\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "# Show the result\n",
        "top_physicians.show()"
      ],
      "metadata": {
        "id": "NCGb9oHqRe4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the results, the physician who received the highest total payment amount is Stephen Burkhart. He is followed by William Binder, and in third place is Kevin Foley. Next is Ivan Osorio, followed by George Maxwell.\n",
        "\n",
        "Robert Booth ranks sixth, followed by Neal ElAttrache in seventh place. Aaron Rosenberg is eighth on the list, while Roger Jackson is ninth. Finally, Peter Bonutti completes the top ten."
      ],
      "metadata": {
        "id": "OHC9iuubRhBZ"
      }
    }
  ]
}